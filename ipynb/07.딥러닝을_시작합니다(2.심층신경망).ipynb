{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ced2c8b1-6bdc-41b6-82da-ccec3eb8fbf0",
   "metadata": {},
   "source": [
    "# 07-2 심층 신경망\n",
    "##### 인공 신경망에 층을 여러 개 추가하여 패션 MNIST 데이터셋을 분류하면서 케라스로 심층 신경망을 만드는 방법을 알아보자\n",
    "\n",
    "- 1절에서 만들었던 인공 신경망의 성능을 더 높여보자\n",
    "\n",
    "### 2개의 층\n",
    "- 케라스 API에서 MNIST 데이터셋을 불러오자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2407e641-a071-4ab6-b28a-447907970ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 13:39:20.506731: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cbf110-46fa-4d28-9c49-9e5175aa93b6",
   "metadata": {},
   "source": [
    "- 픽셀값을 0 ~ 255 범위에서 0 ~ 1 사이로 변환\n",
    "- 28 X 28크기의 2차원 배열을 784 크기의 1차원 배열로 변경\n",
    "- train_test_split() 함수로 훈련 세트와 검증 세트를 나눔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed8e839-e4df-47e7-b12e-e5449b2d9dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_scaled = train_input / 255.0\n",
    "train_scaled = train_scaled.reshape(-1, 28*28)\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef2a257-0d18-49bf-82e1-0d072149ca71",
   "metadata": {},
   "source": [
    "- 이제 인공 신경망 모델에 층을 2개 추가\n",
    "- 만들어진 모델의 대략적인 구조는 아래와 같음\n",
    "![심층신경망](./images/dnn-367-1.jpg)\n",
    "- 인공 신경망에서 입력층과 출력층 사이에 은닉층(hidden layer)이 추가됨\n",
    "- 활성화 함수 : 신경망 층의 선형 방정식의 계산 값에 적용하는 함수 (소프트맥스 함수 역시 활성화 함수)\n",
    "  - 출력층에 적용하는 활성화 함수는 종류가 제한되어 있음 (이진분류 : 시그모이드, 다중 분류 : 소프트맥스)\n",
    "  - 은닉층의 활성화 함순는 사용이 자유로움. 시그모이드 함수와 렐루(ReLU) 함수 등을 사용\n",
    "- 은닉층에 활성화 함수를 적용하는 이유\n",
    "  - 선형 방정식에서 b를 치환시켜 b의 역할을 제거 :\n",
    "    - $a \\times 4 + 2 = b$\n",
    "    - $b \\times 3 - 5 = c $\n",
    "    - $a \\times 12 + 1 = c$\n",
    "  - 신경망에서도 마찬가지임. 은닉층에서 선형적인 산술 계산만 수행한다면 수행 역할이 없는 셈\n",
    "  - 선형 계산을 적당히 비선형으로 비틀어주어야 함\n",
    "    - 다음 층의 계산과 단순히 합쳐지지 않고 나름의 역할을 수행하기 위함 (아래의 식과 같음)\n",
    "    - $a \\times 4 + 2 = b$\n",
    "    - $log(b) = k$\n",
    "    - $k \\times 3 - 5 = c$\n",
    "    - 인공 신경망을 그림으로 나타낼 때 활성화 함수를 생략하는 경우가 많음 (활성화 함수를 층에 포함되어 있다고 간주하기 떄문)\n",
    "- 많이 사용하는 활성화 함수 중 하나는 4장에서 배웠던 시그모이드 함수\n",
    "  - 뉴런의 출력 z 값을 0과 1 사이로 압축\n",
    "- keras로 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae790fbc-ca63-49ca-8b34-605a910a9555",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))\n",
    "dense2 = keras.layers.Dense(10, activation='softmax')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d611a6f5-8fd7-47b4-90fd-58423a80016f",
   "metadata": {},
   "source": [
    "- dense1은 은닉층이고 100개의 뉴런을 가진 밀집층. 은닉층 뉴런 개수를 정하는데에는 특별한 기준이 없음. 뉴런의 적절한 개수 판단은 상당한 경험이 필요함\n",
    "- 한가지 제약 사항은 출력층의 뉴런보다는 많게 만들어야 함 (클래스 10개 확률을 예측하는데 이전 은닉층의 뉴런 10개보다 적다면 정보가 부족할 것)\n",
    "- dense2는 출력층. 10개의 클래스를 분류하므로 10개의 뉴런을 두었고 활성화 함수는 소프트맥스로 지정\n",
    "\n",
    "### 심층 신경망 만들기\n",
    "- 앞서 만든 dense1, dense2를 Sequential 클래스에 추가하여 심층 신경망(DNN)을 만들어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "694339d5-1fe6-47eb-b95e-6f2a6cb1afc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력층(dense2)을 리스트의 가장 마지막에 둘 것!\n",
    "model = keras.Sequential([dense1, dense2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed740d7f-d3cd-4247-a581-e2ad0ca40e1f",
   "metadata": {},
   "source": [
    "- 리스트는 가장 처음 등장하는 은닉층에서 자미작 출력층 순서로 나열해야 함\n",
    "- 인공 신경망의 강력한 성능은 층을 추가하며 입력 데이터에 대해 연속적인 학습을 진행하는 능력에서 나옴\n",
    "- 케라스는 모델의 summary() 메서드를 호출하면 층에 대한 유용한 정보를 얻을 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9947c72b-e56c-46fd-93e9-575cdcd73680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f5c2e-bb55-4108-a6c4-3e2461462f93",
   "metadata": {},
   "source": [
    "- 첫 줄은 모델 이름\n",
    "- 이후 모델에 들어있는 층이 순서대로 나열\n",
    "- 순서는 리스트에 입력된 순서로 나열\n",
    "- 층마다 층 이름, 클래스, 출력 크기, 모델 파라미터 개수가 출력\n",
    "- 출력 크기(None, 100)\n",
    "  - 샘플 개수(None) -> 케라스 모델의 fit() 메서드에 훈련 데이터를 주입하면 데이터를 한 번에 모두 사용하지 않고 잘게 나누어 여러 번에 걸쳐 경사 하강법 단계를 수행 (미니배치 경사 하강법)\n",
    "  - 케라스 기본 미니배치 크기는 32개. 이 값은 fit() 메서드에 batch_size 매개변수로 바꿀 수 있음\n",
    "  - 샘플 개수를 고정하지 않고 어떤 배치 크기에도 유연하게 대응할 수 있도록 None으로 설정\n",
    "  - 신경망 층에 입력되거나 출력되는 배열의 첫 번째 차원을 배치 차원이라고 부름\n",
    "  - 두번째 출력은 : 뉴런 개수(100) -> 샘플마다 784개의 픽셀값이 은닉층을 통과하며 100개의 특성으로 압축\n",
    "  - 마지막으로 모델 파라미터 개수 출력(Param, 78500) : Dense 층이므로 입력 픽셀 784개와 100개의 모든 조합에 대한 가중치가 있음 + 뉴런마다 1개의 절편\n",
    "![Param-1](./images/dnn-373-1.jpg)\n",
    "- 두 번째 층의 출력 크기는 (None, 10) : 배치 차원은 동일하게 None이고 출력 뉴런 개수가 10개임\n",
    "- 이 층의 모델 파라미터 개수는 100개의 은닉층 뉴런과 10개의 출력층 뉴런이 모두 연결되고 출력층의 뉴런마다 하나의 절편이 있기 때문에 1,010개의 모델 파라미터가 있음\n",
    "![Param-2](./images/dnn-373-1.jpg)\n",
    "\n",
    "- summary() 메서드의 마지막에는 총 모델 파라미터 개수와 훈련 파라미터 개수가 동일하게 79,510개 -> 은닉층과 출력층의파라미터 개수를 합친 값\n",
    "- 훈련되지 않는 파라미터(Non-trainable params)는 0\n",
    "  - 간혹 경사 하강법으로 훈련되지 않는 파라미터를 가진 층이 있음\n",
    "  - 이런 층의 파라미터 개수가 여기 나타남\n",
    "\n",
    "### 층을 추가하는 다른 방법\n",
    "- 매개변수를 추가하여 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f072f37a-4249-4747-8cd2-88b36f2bd59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'),\n",
    "                          keras.layers.Dense(10, activation='softmax', name='output')], name='패션 MNIST 모델')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f570ce30-0e7f-4350-b7ac-aaeed5f3e90b",
   "metadata": {},
   "source": [
    "- 해당 코드는 추가되는 층을 한눈에 알아보는 장점이 있음\n",
    "- 모델 이름과 층은 반드시 영문이어야 함\n",
    "- summary() 메서드 출력에서 확인 고고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6916e784-2820-46bf-badf-b91014e953ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"패션 MNIST 모델\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hidden (Dense)              (None, 100)               78500     \n",
      "                                                                 \n",
      " output (Dense)              (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f911f18-9133-4e83-ae36-8d6c7b785595",
   "metadata": {},
   "source": [
    "- 이 방법이 편리하지만 많은 충을 추가하려면 코드가 너무 길어짐\n",
    "- 이럴땐 add() 함수 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed08adf6-092b-4aa0-9a76-1886d14199d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10fe6f00-1d74-4795-aa87-1e536f9863f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 이 방법은 한눈에 층을 볼 수 있고 프로그램 실행 시 동적으로 층을 선택하여 추가 가능\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a098c-6693-4c37-b112-42258d6badb2",
   "metadata": {},
   "source": [
    "- 이제 모델을 훈련해보자\n",
    "- compile() 메서드 설정은 1절에서 했던 것과 동일\n",
    "- 5번의 에포크 동안 훈련해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64bfc4f2-113f-4c65-9c79-b533f753139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5704 - accuracy: 0.8051\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.4129 - accuracy: 0.8524\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3766 - accuracy: 0.8627\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3546 - accuracy: 0.8704\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3360 - accuracy: 0.8783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x13297c990>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_scaled, train_target, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22445ed-626f-463c-9d22-65cd91e64864",
   "metadata": {},
   "source": [
    "- 훈련 세트에 대한 성능을 보면 추가된 층이 성능을 향상시켰다는 것을 알 수 있음\n",
    "- 인공 신경망에 몇 개의 층을 추가하더라도 compile()메서드와 fit() 메서드의 사용은 동일 (keras API 장점)\n",
    "- 이미지 분류에서 높은 성능을 낼 수 있는 활성화 함수에 대해 알아보자\n",
    "\n",
    "### 렐루 함수\n",
    "- 초창기 은닉층에 많이 사용된 함수는 시그모이드였음\n",
    "- 시그모이드 단점 -> 오른쪽과 왼쪽 끝으로 갈수록 평행해짐 -> 신속한 대응 못함\n",
    "- 심층신경망 -> 효과가 누적되어 학습이 어려움\n",
    "- 렐루 함수 -> 입력이 양숟일 경우 함수가 없는 것처럼 입력을 통과, 음수일 경우 0\n",
    "- 시그모이드\n",
    "![시그모이드](./images/dnn-377-1.jpg)\n",
    "- 렐루\n",
    "![렐루](./images/dnn-377-2.jpg)\n",
    "- 렐루 함수는 maz(0,z)와 같이 쓸 수 있음\n",
    "- z가 0 > z이면 z 출력 <=이면 0 출력, 렐루는 이미지 처리에서 좋은 성능을 나타냄\n",
    "- 패션 MNIST 데이터는  28 X 28 크기를 1차원으로 변환하기 위해 reshape()메서드를 사용했음\n",
    "  - 케라스에서 Flatten 층을 제공\n",
    "  - Flatten 클래스는 배치 차원을 제외하고 나머지 입력 차원을 모두 일렬로 펼치는 역할(입력에 곱해지는 가중치, 절편이 없음)\n",
    "  - Flatten 클래스를 층처럼 입력층과 은닉층에 추가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9796df5-9f51-482c-b185-4a4ba12b59ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28, 28)))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788770b-8658-4858-87ce-b26935d9325c",
   "metadata": {},
   "source": [
    "- Dense 층에 있던 input_shape 매개변수를 Flatten 층으로 옮김\n",
    "- Dense 층 활성화 함수를 relu로 변경\n",
    "- 신경망 깊이가 3인 신경망이라고 부르지는 않음 (Flatten은 학습하는 층이 아니기 떄문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "111a5fbb-d384-4906-9cf4-407a98f1a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               78500     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 79510 (310.59 KB)\n",
      "Trainable params: 79510 (310.59 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621331de-6357-4c60-9cb4-7753aa913b2b",
   "metadata": {},
   "source": [
    "- 첫번째 Flatten 클래스에 포함된 모델 파라미터는 0\n",
    "  - 앞의 출력에서 784개의 입력이 첫 번째 은닉층에 전달됨을 알 수 있음\n",
    "- 입력 데이터에 대한 전처리 과정을 가능한 모델에 포함시키는 것이 케라스 API의 철학 중 하나\n",
    "- 훈렌 데이터를 다시 준비하여 모델 훈련해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e9410a1-5be6-4c5b-8dcc-37d867d01b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()\n",
    "train_scaled = train_input / 255.0\n",
    "train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31a6e71c-415a-43cb-86cf-5a5d513de272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.5317 - accuracy: 0.8119\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3931 - accuracy: 0.8597\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3523 - accuracy: 0.8728\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3309 - accuracy: 0.8809\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3166 - accuracy: 0.8867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1402030d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 컴파일 및 훈련\n",
    "model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_scaled, train_target, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04df3784-b377-4478-93b6-573f14cdab4d",
   "metadata": {},
   "source": [
    "- 시그모이드 함수와 비교했을 떄 성능 향상을 볼 수 있음\n",
    "- 검증 세트도 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c29814b-bbd2-4d8b-b262-9573f135508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 991us/step - loss: 0.3534 - accuracy: 0.8771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3533776104450226, 0.8770833611488342]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70571f91-9e49-441d-b49c-b6b5ab77542b",
   "metadata": {},
   "source": [
    "- 은닉층을 추가하여 2-3%의 성능 향상을 확인\n",
    "- 이제 인공 신경망의 하이퍼파라미터에 대해 알아보자\n",
    "\n",
    "### 옵티마이저\n",
    "- 신경망에는 하이퍼파라미터가 많음\n",
    "- 하이퍼파라미터\n",
    "  - 추가할 은닉층의 개수\n",
    "  - 은닉틍의 뉴런 개수\n",
    "  - 활성화 함수\n",
    "  - 층의 종류 (밀집층이 아닌 다른 종류의 층을 선택할 수도 있음)\n",
    "  - fit() 메서드의 batch_size 매개변수 (기본은 미니배치 경사 하강법, 디폴트는 32개)\n",
    "  - fit() 메서드의 epochs 매개변수\n",
    "  - 옵티마이저 : compile() 메서드에서 경사 하강법 종류(위에서는 케라스 기본 경사 하강법 알고리즘인 RMSprop 사용)\n",
    "  - 옵티마이저의 학습률 (RMSprop)\n",
    "\n",
    "#### 옵티마이저 테스트\n",
    "- SGD 옵티마이저"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d67a3010-a5f1-491a-8346-4abba5e89a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0333ef39-72da-4a0a-a9dd-4ecc8af4c3da",
   "metadata": {},
   "source": [
    "- 이 옵티마이저는 tensorflow.keras.optimizers 패키키지의 SGD 클래스로 구현되어 있음\n",
    "- 'sgd' 문자열은 이 클래스의 기본 설정 매개변수로 생성한 객체와 동일 - 다음 코드는 위의 코드와 동일한 코드\n",
    "```python\n",
    "sgd = keras.optimizer.SGD()\n",
    "model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "```\n",
    "- SGD 클래스의 학습율 기본값이 0.01일 때 이를 바꾸고 싶다면 learning_rate 매개변수에 지정\n",
    "```python\n",
    "sgd = keras.optimizer.SGD(learning_rate=0.1)\n",
    "```\n",
    "- 다양한 옵티마이저 옵션을 확인해보자\n",
    "![옵티마이저](./images/dnn-382-1.jpg)\n",
    "\n",
    "- 기본 경사 하강법 옵티마이저는 모두 SGD 클래스에서 제공\n",
    "- 모멘턴 최적화(momentum optimization) : 그래디언트 가속도처럼 사용(SGD 클래스의 momentum 매개변수 기본값은 0보다 큰 값으로 지정)\n",
    "  - 보통 momentum 매개변수는 0.9\n",
    "- \b네스테로프 모멘텀 최적화(네스테로프 가속 경사, nesterov momentum optimization) : SGD 클래스의 nesterov 매개변수를 True로 지정(기본값 False)\n",
    "  - 모멘텀 최적화를 2번 반복하여 구현\n",
    "  - 대부분의 경우 네스테로프 모멘텀 최적화가 기본 확률적 경사 하강법보다 더 나은 성능 제공\n",
    "\n",
    "```python\n",
    "sgd = keras.optimizers.SGD(momentum=0.9, nesterov=True)\n",
    "```\n",
    "\n",
    "- 적응적 학습률(adaptive learning rate) : 모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있음. (안정적으로 최적점에 수렴할 가능성이 높음)\n",
    "  - 적응적 학습률을 사용하는 대표 옵티마이저는 Adagrad와 RMSprop\n",
    "  - compile() 메서드 optimizer 매개변수의 기본값이 바로 'rmsprop'\n",
    "  - 매개변수 변경을 위해서는 Adagrad, RMSprop 클래스 객체를 만들어 사용\n",
    "\n",
    "```python\n",
    "# Adagrad\n",
    "adagrad = keras.optimizers.Adagrad()\n",
    "model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "\n",
    "# RMSprop\n",
    "rmsprop = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "```\n",
    "\n",
    "- 모멘텀 최적화와 RMSprop의 장점을 접목한 것이 Adam\n",
    "  - Adam은 RMSprop과 함께 맨처음 시도해 볼 수 있는 좋은 알고리즘\n",
    "  - Adam 클래스도 keras.optimizers 패키지 아래에 있음\n",
    "- 적응적 학습률을 사용하는 이 3개의 클래스는 learning_rate 매개변수 기본값이 모두 0.001\n",
    "- 옵티마이저 작동 방식은 핸즈온 머신러닝 2판 참고\n",
    "- Adam 클래스의 매개변수 기본값을 사용해 패션 MNIST 모델 훈련해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44f081e3-8488-4139-a9e5-1debefcfb425",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=(28,28)))\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d0811a-5547-41fa-9342-c5c90c55dfff",
   "metadata": {},
   "source": [
    "- compile() 메서드 옵티마이저 adam으로 설정 후 5번의 에포크 동안 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94a1e767-e989-4390-a87a-4c1c429f1624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1500/1500 [==============================] - 3s 1ms/step - loss: 0.5251 - accuracy: 0.8154\n",
      "Epoch 2/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3939 - accuracy: 0.8591\n",
      "Epoch 3/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3531 - accuracy: 0.8710\n",
      "Epoch 4/5\n",
      "1500/1500 [==============================] - 2s 1ms/step - loss: 0.3279 - accuracy: 0.8805\n",
      "Epoch 5/5\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.3095 - accuracy: 0.8863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1405fee50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')\n",
    "model.fit(train_scaled, train_target, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bdb24c-4fe7-4938-bb80-3f4b64067d76",
   "metadata": {},
   "source": [
    "- RMSprop을 사용했을 때와 거의 같은 성능\n",
    "- 검증세트로 확인해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27d27f24-55d7-47f3-a574-2f9101b8c5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375/375 [==============================] - 0s 942us/step - loss: 0.3355 - accuracy: 0.8802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3354775905609131, 0.8801666498184204]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_scaled, val_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
