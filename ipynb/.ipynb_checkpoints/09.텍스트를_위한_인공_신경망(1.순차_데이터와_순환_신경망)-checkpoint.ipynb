{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e278b668-2fc4-4c11-b9eb-b82c2b8a0079",
   "metadata": {},
   "source": [
    "# 09-1 순차 데이터와 순환 신경망\n",
    "##### 순차 데이터의 특징을 알고 순환 신경망의 개념을 학습합니다.\n",
    "\n",
    "- 댓글을 분석해 긍정, 부정 평가 판단\n",
    "\n",
    "### 순차 데이터\n",
    "- 순차 데이터(sequential data) : 텍스트나 시계열 데이터(time series data)와 같이 순서에 의미가 있는 데이터\n",
    "  - 문장에는 순서가 있으니 단어의 순서가 중요한 순차 데이터임\n",
    "\n",
    "- 피드포워드 신경망(FFNN) : 입력 데이터의 흐름이 앞으로만 전달되는 신경망\n",
    "  - 완전 연결 신경망, 합성곱 신경망 등에 해당하며 정방향 계산을 수행하면 해당 샘플은 버려짐\n",
    "\n",
    "### 순환 신경망\n",
    "- 순환 신경망(recurrent neural network, RNN) : 일반적인 완전 영결 신경망과 비슷\n",
    "  - 완전 연결 신경망에 이전 데이터의 처리 흐름을 순환하는 고리를 추가\n",
    "\n",
    "![rnn-1](./images/rnn-489-1.jpg)\n",
    "\n",
    "- 뉴런의 출력이 다시 자기 자신으로 전달됨. (샘플 처리 시 이전 데이터 재사용)\n",
    "\n",
    "- 예) A, B, C 3개의 샘플을 처리한다 가정. O는 출력 결과\n",
    "  - 첫 번째 샘플 A를 처리하고 난 출력(O<sub>A</sub>)이 다시 뉴런으로 들어감\n",
    "\n",
    "    ![rnn-2](./images/rnn-489-2.jpg)\n",
    "\n",
    "  - 그 다음 B 처리 시 입력 값에 O<sub>A</sub>를 함꼐 사용\n",
    "\n",
    "    ![rnn-3](./images/rnn-489-3.jpg)\n",
    "\n",
    "  - C를 처리할 때는 O<sub>B</sub>를 함께 사용\n",
    " \n",
    "    ![rnn-4](./images/rnn-490-1.jpg)\n",
    "\n",
    "- O<sub>B</sub>와 C를 사용해 만든 O<sub>\bC</sub>는 B와 A에 대한 정보가 담겨있다 말할 수 있음\n",
    "  - 물론 O<sub>C</sub>에는 A 정보보다 B 정보가 더 많을 것\n",
    "\n",
    "- 타임스텝(timestep) : 위와 같이 샘플을 처리하는 한 단계를 말함\n",
    "- 셀(cell) : 순환 신경망의 층을 말함\n",
    "  - 한 셀에는 여러 개의 뉴런이 있지만 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현\n",
    "  - 셀의 출력을 **은닉 상태(hidden state)** 라고 부름\n",
    "  - 합성곱 신경망에서처럼 신경망 구조마다 부르는 이름이 조금씩 다르지만 기본 구조는 같음\n",
    "\n",
    "- 달라지는 것은 층 출력 (즉 은닉 상태)을 다음 타임 스텝에 재사용\n",
    "\n",
    "![rnn-5](./images/rnn-490-2.jpg)\n",
    "\n",
    "- 일반적인 은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트(hyperbolic tangent) 함수인 tanh(텐에이치)가 많이 사용됨\n",
    "  - 시그모이드 함수와 달리 -1 ~ 1 사이의 범위를 가짐\n",
    "    ![rnn-6](./images/rnn-491-1.jpg)\n",
    " \n",
    "  - 순환 신경망에도 활성화 함수가 반드시 필요하다는 것을 기억하자\n",
    "\n",
    "- 순환 신경망의 뉴런은 입력과 가중치를 곱하고 바로 이전 타임스텝의 은닉 상태에 곱해지는 가중치를 곱함\n",
    "- 아래의 그림은 2개의 가중치를 셀 안에서 구분해서 표시\n",
    "  - w<sub>x</sub>는 입력에 곱해지는 가중치\n",
    "  - w<sub>b</sub>는 이전 타입스텝의 은닉 상태에 곱해지는 가중치\n",
    "  - 뉴런마다 절편이 있음 (그림에서는 표시하지 않음)\n",
    "    ![rnn-7](./images/rnn-492-1.jpg)\n",
    "\n",
    "  - 아래의 그림츨 타임스텝으로 펼쳤다고 말함\n",
    "    ![rnn-8](./images/rnn-492-2.jpg)\n",
    "\n",
    "  - 타입스텝 1에서 셀의 출력 h<sub>1</sub>이 타입스텝 2의 셀로 주입\n",
    "    - 이때 w<sub>h</sub>와 곱해짐\n",
    "    - 과정의 반복\n",
    " - 모든 타입스텝에서 사용되는 가중치는 w<sub>h</sub> 하나임\n",
    " - 가중치 w<sub>h</sub>는 타임스텝에 따라 변화되는 뉴런의 출력을 학습\n",
    " - 그럼 맨 처음 타임스텝 1에서 사용되는 이전 은닉 상태 h<sub>0</sub>은 이전 상태가 없으므로 0으로 초기화\n",
    "\n",
    "### 셀의 가중치와 입출력\n",
    "- 순환 신경망의 셀에서 필요한 가중치 크기 계산\n",
    "- 복잡한 모델을 배울수록 가중치 개수를 계산하면 잘 이해하고 있는지 알 수 있음\n",
    "\n",
    "- 예를 들어 아래 그림처럼 순환층에 입력되는 특성의 개수가 4개이고 순환층 뉴런이 3개라 가정\n",
    "  - w<sub>x</sub> 크기는 4 X 3 = 12개가 됨\n",
    "    ![rnn-8](./images/rnn-492-2.jpg)\n",
    "\n",
    "  - 순환층에서 다음 타임스텝에 재사용되는 은닉 상태를 위한 가중치 w<sub>b</sub>의 크기는 어떻게 될까?\n",
    "    ![rnn-8](./images/rnn-492-2.jpg)\n",
    "\n",
    "  - 순환층에 있는 첫 번째 뉴런(r<sub>1</sub>)의 은닉 상태가 다음 타임스텝에 재사용 될 떄 첫 번째 뉴런과 두 번째 뉴런, 세 번째 뉴런에 모두 전달 (위 그림에서 붉은색 표시)\n",
    "  - 두 번째 은닉 상태도 1, 2, 3 번째 뉴런에 모두 전달(파란 화살표)\n",
    "  - 세 번쨰 은닉 상태도 동일 (검은 화살표)\n",
    "  - 이 순환층에서 은닉 상태를 위한 가중치 w<sub>b</sub>는 3 X 3 = 9개\n",
    " \n",
    "  - 이제 모델 파라미터를 구해보면 (가중치에 절편을 더함)\n",
    "    - 각 뉴런마다 하나의 절편이 있으므로 이 순환층은 모두 (12 + 9 + 3)인 24개의 모델 파라미터를 가지고 있음\n",
    "      $$ 모델 파라미터 수 = w_{x} + w_{b} + 절편 = 12 + 9 + 3 = 24 $$\n",
    "\n",
    "- 순환층의 가중치 크기를 알아보았으므로 이번에는 순환층의 입출력에 대해 알아보자\n",
    "  - 순환층은 일반적으로 샘플마다 2개의 차원을 가짐 (하나의 샘플을 시퀀스(sequence)라고 말함)\n",
    "  - 시퀀스의 길이가 타임스텝의 길이가 됨\n",
    "  - i am a boy라는 문장을 3개의 숫자로 표현한다고 가정 (다음 절에서 숫자 표현 의미 설명)\n",
    "    ![rnn-9](./images/rnn-494-1.jpg)\n",
    "\n",
    "  - 위의 입력이 순환층을 통과하면 2, 3 번째 차원이 사라지고 순환층 뉴런 개수만큼 출력됨\n",
    "    ![rnn-10](./images/rnn-494-2.jpg)\n",
    "\n",
    "  - 하나의 샘플은 시퀀스 길이(단어 개수)와 단어 표현의 2차원 배열\n",
    "  - 순환층을 통과하면 1차원 배열로 변경됨\n",
    "  - 이 1차원 배열의 크기는 순환층의 뉴런 개수에 의해 결정\n",
    "    - 순환층은 기본적으로 마지막 타임스텝의 은닉 상태만 출력으로 내보냄\n",
    "    - 아래 그림에서 점선으로 표시 했고, 마지막 타임스탭의 은닉 상태임을 나타내기 위해 아랫첨자 f 사용\n",
    "      ![rnn-11](./images/rnn-495-1.jpg)\n",
    "\n",
    "    - 입력된 시퀀스 길이를 모두 읽어서 정보를 마지막 은닉 상태에 압축하여 전달하는 것처럼 볼 수 있음\n",
    "\n",
    "- 순환 신뎡망도 여러 개의 층을 쌓을 수 있음\n",
    "![rnn-12](./images/rnn-495-2.jpg)\n",
    "\n",
    "- 첫 번째 셀은 모든 타입스텝의 은닉 상태를 출력하고, 두 번째 셀은 마지막 타임스텝의 은닉 상태만 출력\n",
    "\n",
    "- 마지막으로 출력층 구성을 알아보자\n",
    "- 순환 신경망도 마지막에는 밀집층을 두어 클래스 분류\n",
    "  - 다중 분류 : 출력층에 클래스 개수만큼 뉴런을 두고 소프트맥스 활성화 함수를 사용\n",
    "  - 이진 분류 : 하나의 뉴런을 두고 시그모이드 활성화 함수 사용\n",
    "- 다중 분류 문제에서 입력 샘플 크기가 (20, 100)일 경우 하나의 순환층을 사용하는 순환 신경망의 구조는 아래와 같음\n",
    "![rnn-13](./images/rnn-496-1.jpg)\n",
    "\n",
    "- 위 그림에서 샘플은 20개 타입스텝으로 이루어져 있음\n",
    "- 각 타임스텝은 100개 표현 또는 특성으로 구성. 은닉 상태 크기는 (10,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
